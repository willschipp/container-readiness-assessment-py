## Standalone

Use the [deployment](./deployment.yaml) and [service](./service.yaml) to deploy a single instance of the codellama model served by Llama.cpp